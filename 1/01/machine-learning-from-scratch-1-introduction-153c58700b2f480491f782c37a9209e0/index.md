# 

# Machine Learning from Scratch - 1: Introduction

ë§ˆê°ì¼: 7
ëª©í‘œ ë°œí–‰ì¼ì: 2021ë…„ 1ì›” 10ì¼
ìƒíƒœ: ì§„í–‰ ì¤‘
ìƒì„±ì¼: 2021ë…„ 1ì›” 2ì¼ ì˜¤í›„ 3:25
ì¹´í…Œê³ ë¦¬: Machine Learning
íƒœê·¸: Gradient Descent, Linear Algebra

[0xb270/MachineLearning](https://github.com/0xb270/MachineLearning/blob/master/ml/%5BML%5DCH1_Numpy_and_Pandas.ipynb)

# ì‹œë¦¬ì¦ˆë¥¼ ì‹œì‘í•˜ë©´ì„œ

ë¨¸ì‹ ëŸ¬ë‹ì˜ ê°œë…ì„ ì²˜ìŒ ì ‘í•œ ê²ƒì€ ëŒ€í•™ì— ì…í•™í–ˆë˜ 2016ë…„ë„ ì˜€ê³ , êµ¬ê¸€ [DeepMind](https://deepmind.com/) ì‚¬ì˜ AlphaGoì™€ ì´ì„¸ëŒì˜ ëŒ€êµ­ì´ ì´ë£¨ì–´ì§€ê³  ìˆì—ˆìŠµë‹ˆë‹¤. ê³¼ê±°ì˜ ë°ì´í„°ì—ì„œ ì•”ë¬µì ì¸ íŒ¨í„´ê³¼ ê·œì¹™ì„ í•™ìŠµí•˜ê³ , ìƒˆë¡œìš´ ë°ì´í„°ì— ì´ë¥¼ ì ìš©í•˜ì—¬ ì¦‰ê°ì ìœ¼ë¡œ ì¶©ë¶„í•œ ëŒ€ë‹µì„ ì¶”ë¡ í•œë‹¤ëŠ” ì•„ì´ë””ì–´ëŠ” ëª…ì‹œì ì¸ ì§€ì‹ë§Œì„ ìŠµë“í•´ì™”ë˜ ì €ì—ê²Œ ìˆì–´ì„œ ì •ë§ ë†€ë¼ì› ìŠµë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì–´ë–»ê²Œ í•˜ë©´ ê³¼ê±°ì˜ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ëª¨ë¸ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ì„œ ê°•ì˜, ì±…, ìŠ¤í„°ë””, ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë“±ì„ ì „ì „í•˜ë©° ì¡°ê¸ˆì”© ê³µë¶€í•´ì™”ìœ¼ë‚˜, ì •ëˆëœ ìë£Œì˜ í˜•íƒœë¡œ ê¸°ë¡ë˜ì–´ ìˆì§€ ì•Šì•„ì„œ í•­ìƒ ì•„ì‰¬ì›€ì´ ë‚¨ê³ ëŠ” í–ˆìŠµë‹ˆë‹¤. 

ì´ ì¼ë ¨ì˜ ***Machine Learning from Scratch*** ì‹œë¦¬ì¦ˆëŠ” ì•ì„  ê°œì¸ì ì¸ ê³ ë¯¼ì„ í•´ì†Œí•˜ê³ , ì´ë¯¸ ì¢‹ì€ ìë£Œê°€ ë§ì´ ê³µìœ ë˜ì–´ ìˆì§€ë§Œ ì¡°ê¸ˆì´ë¼ë„ ë¨¸ì‹ ëŸ¬ë‹ì„ ìƒˆë¡­ê²Œ í˜¹ì€ ê³„ì†í•´ì„œ ë°°ìš°ê³  ìˆìœ¼ì‹  ë¶„ë“¤ì—ê²Œ ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ë§ˆìŒìœ¼ë¡œ ì—°ì¬ë¥¼ ì‹œì‘í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì—°ì¬ì˜ ì „ì²´ì ì¸ ìˆœì„œëŠ” ë¨¸ì‹  ëŸ¬ë‹ì„ ë°°ìš°ëŠ”ë° ìˆì–´ì„œ ê°€ì¥ ê¸°ì´ˆê°€ ëœë‹¤ê³  í•  ìˆ˜ ìˆëŠ” [Coursera](https://www.coursera.org/) ì•¤ë“œë¥˜ ì‘ êµìˆ˜ë‹˜ì˜ ê°•ì˜, ***[Machine Learning](https://www.coursera.org/learn/machine-learning)***ì„ ê¸°ë°˜ìœ¼ë¡œ í•  ê²ƒì´ë©°, ëŒ€í•™ì—ì„œ ê°•ì˜ë¥¼ ë“¤ì—ˆë˜ ì´ì¬ì‹ êµìˆ˜ë‹˜ì˜ ì €ì„œ [ë°ì´í„° ì• ë„ë¦¬í‹±ìŠ¤](https://wikibook.co.kr/data-analytics/)(2020, ìœ„í‚¤ë¶ìŠ¤)ì˜ ë‚´ìš©ë„ í•¨ê»˜ ì •ë¦¬í•˜ê³ ì í•©ë‹ˆë‹¤. 

 ì €ë„ ê³µë¶€í•˜ëŠ” ê³„ì†í•´ì„œ ê³µë¶€í•˜ëŠ” ì…ì¥ì´ê¸°ì— ì œê°€ ì•„ëŠ” ì„ ì—ì„œëŠ” ìµœëŒ€í•œ ìˆ˜ë¦¬ì ì¸ ë‚´ìš©(e.g., ì„ í˜•ëŒ€ìˆ˜, í†µê³„ ë“±ë“±)ì„ ë‹´ì„ ìˆ˜ ìˆê³ ì í•˜ì˜€ê³ , ì´ë¶€ë¶„ì´ ì•„ë‹ˆë”ë¼ë„ Python `numpy`ë¥¼ ì´ìš©í•œ êµ¬í˜„ì´ë‚˜ ëª¨ë¸ì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•œ ê²½ì˜í•™ì  í•¨ì˜ë¥¼ ê¸°ìˆ í•˜ì—¬ ì´ ê¸€ì„ ì½ê³  ê³„ì‹  ì—¬ëŸ¬ ë¶„ì•¼ì˜ ë…ì ë¶„ë“¤ê»˜ ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.

P.S. ë°‘ë°”ë‹¥ ë¶€í„° ì‹œì‘í•˜ëŠ”(from Scratch)ë¼ëŠ” ëª…ì¹­ì€ ì´ë¯¸ ë‹¤ë“¤ ì•„ì‹œê² ì§€ë§Œ, ì œê°€ ê°œì¸ì ìœ¼ë¡œ ì¢‹ì•„í•˜ëŠ” ì‹œë¦¬ì¦ˆì¸  O'reilly ì‚¬ì˜ [ë°‘ë°”ë‹¥ ë¶€í„° ì‹œì‘í•˜ëŠ” ë°ì´í„°ê³¼í•™(Data Science from Scratch)](https://blog.insightbook.co.kr/2016/05/27/%eb%8d%b0%ec%9d%b4%ed%84%b0-%ea%b3%bc%ed%95%99%ec%97%90-%ed%95%84%ec%9a%94%ed%95%9c-%ea%b8%b0%ec%b4%88-%ec%9d%b4%eb%a1%a0%ea%b3%bc-%ed%94%84%eb%a1%9c%ea%b7%b8%eb%9e%98%eb%b0%8d-%eb%91%90-%eb%a7%88/), [ë°‘ë°”ë‹¥ ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹(Deep Learning from Scratch)](https://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198)ë¥¼ íŒ¨ëŸ¬ë””í•œ ê²ƒ ì…ë‹ˆë‹¤.

## 1. Machine Learning with `numpy`

> `numpy`ëŠ” ìˆ˜í•™ ë° ê³¼í•™ ì—°ì‚°ì„ ìœ„í•œ Pythonì˜ ê¸°ë³¸ íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤. [[ê³µì‹ í™ˆí˜ì´ì§€]](https://numpy.org/)

ì½”ì„¸ë¼ Machine Learning ìˆ˜ì—…ì˜ ê²½ìš°, ìˆ˜ì¹˜ê³„ì‚°ìš© ì–¸ì–´ì¸ [MATLAB](https://www.mathworks.com/products/matlab.html) í˜¹ì€ [Octave](https://www.gnu.org/software/octave/index)ë¡œ ê³¼ì œë¥¼ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. **ë²¡í„°** ë° **í–‰ë ¬** ì—°ì‚°ì´ ì–¸ì–´ ìì²´ì— ì •ì˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë³„ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  í•„ìš” ì—†ì´ ëª¨ë¸ì˜ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ ë³¸ì§ˆì€ ê²°êµ­ ë²¡í„°ì™€ í–‰ë ¬ ì—°ì‚°ì´ê¸° ë•Œë¬¸ì—, ì–´ë–¤ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ êµ¬í˜„í•˜ì‹œë”ë¼ë„ ì›ë¦¬ë§Œ ì´í•´í•œë‹¤ë©´ ì‰½ê²Œ êµ¬í˜„í•˜ì‹¤ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. 

ë³¸ë˜ ìˆ˜ê°•í• ë•ŒëŠ” ìë£Œì— ë§ì¶”ì–´ Octaveë¡œ êµ¬í˜„í–ˆì—ˆìœ¼ë‚˜, ì´ ì‹œë¦¬ì¦ˆì—ì„œëŠ” Pythonìœ¼ë¡œ êµ¬í˜„í•˜ê³ ì í•©ë‹ˆë‹¤. ê·¸ ì¤‘ `numpy` ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ê²ƒì¸ë°,Â `C`/`C++`,Â `Fortran`ìœ¼ë¡œ ì‘ì„±ë˜ì–´ ë¹ ë¥´ë©´ì„œë„ ì •êµí•œ ìˆ˜í•™ ì—°ì‚°, íŠ¹íˆÂ **ë²¡í„°**Â ë°Â **í–‰ë ¬**(ì„ í˜•ëŒ€ìˆ˜) ì—°ì‚°ì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.      

- `numpy`ì˜ ê°•ì 
    - a powerful N-dimensional array object
    - sophisticated (broadcasting) functions
    - tools for integrating C/C++ and Fortran code
    - useful linear algebra, Fourier transform, and random number capabilities

ë˜í•œ `numpy`ì˜ array ìë£Œí˜•ì€ Pythonì˜ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì–‘ëŒ€ì‚°ë§¥ì´ë¼ í•  ìˆ˜ ìˆëŠ” [Pytorch](https://pytorch.org/)ì˜ tensor ìë£Œí˜•ê³¼ë„ ì‰½ê²Œ ìƒí˜¸ ì „í™˜ì´ ë˜ê¸° ë•Œë¬¸ì— ì‹¤ì§ˆì ì¸ ëª¨ë¸ë§ì—ë„ ë¶€ë¶„ì ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ `numpy` ê¸°ë°˜ì˜ ì—°ì‚°ê³¼ ëª¨ë¸ë§ì„ ê·¸ëŒ€ë¡œ GPU í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ì „í™˜ì‹œí‚¬ ìˆ˜ ìˆëŠ” CUDA ê¸°ë°˜ì˜ [CuPy](https://cupy.dev/)ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ì¡´ì¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë³¸ì¸ì´ ë°‘ë°”ë‹¥ë¶€í„° ìŒ“ì•„ì˜¬ë¦¬ë©° ê³µë¶€í•œ ì½”ë“œë¥¼ ëª‡ì¤„ì˜ ìˆ˜ì •ë§Œìœ¼ë¡œë„ ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. 

## 2. Linear Algebra with `numpy`

### 2.1. `numpy` ê¸°ì´ˆ

ê´€ìŠµì ìœ¼ë¡œ `numpy`ë¥¼ import í• ë•Œ, `np`ë¼ëŠ” ì•½ì–´ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

```python
import numpy as np
```

#### ë°°ì—´ ìƒì„±

`numpy`ì˜ ìë£Œ í´ë˜ìŠ¤ë¥¼ **ndarray**(N-Dimensional Array)ë¼ê³  í•©ë‹ˆë‹¤.

- `np.array()`
    - ê¸°ì¡´ì˜ pythonÂ `list`,Â `tuple`Â ë“±ìœ¼ë¡œ ìƒì„±
    - `dtype`ì„ ì¡°ì •í•˜ì—¬ ë°ì´í„° íƒ€ì… ì§€ì • ê°€ëŠ¥

```python
print(np.array([2,3,4]))                         # ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ìš©í•œ array ìƒì„±
print(np.array([(1.5,2,3), (4,5,6)]))            # íŠœí”Œì„ ì´ìš©í•œ array ìƒì„±

a = np.array( [ [1,2], [3,4] ], dtype=complex)   # ë°ì´í„°íƒ€ì… ì§€ì • (ë³µì†Œìˆ˜)
print(type(a))
```

- `np.arrange()`
    - ë²”ìœ„ë¥¼ ì§€ì •í•˜ì—¬ array ìƒì„±
    - ì²«ë²ˆì§¸ì™€ ë‘ë²ˆì§¸ ì¸ìë¡œ rangeë¥¼ ê²°ì •í•˜ê³ , ì„¸ë²ˆì§¸ ì¸ìë¡œ ê°„ê²©ì„ ê²°ì •

```python
np.arange(10, 30, 5) 
## Output: array([10, 15, 20, 25])  
```

- `np.zeros()` or `np.ones()` or `np.empty()`
    - ì§€ì •ëœ ì‚¬ì´ì¦ˆë§Œí¼ì˜ array ìƒì„± (ìˆœì„œëŒ€ë¡œ ê°ê° 0, 1, ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê°’)

```python
np.zeros((3,4)) # 3í–‰ 4ì—´ í¬ê¸°ì˜ ëª¨ë“  ê°’ì´ 0ì¸ 2ì°¨ì› ë°°ì—´
```

```
# Out
array([[ 0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.]])
```

---

```python
np.ones((2,3,4), dtype=np.int16) # (2, 3, 4) í¬ê¸°ì˜ ëª¨ë“  ê°’ì´ ì •ìˆ˜ 1ì¸ 3ì°¨ì› ë°°ì—´
```

```
# Out
array([[[1, 1, 1, 1],
        [1, 1, 1, 1],
        [1, 1, 1, 1]],

       [[1, 1, 1, 1],
        [1, 1, 1, 1],
        [1, 1, 1, 1]]], dtype=int16)
```

---

```python
np.empty((2,3), dtype=np.double) # (2, 3) í¬ê¸°ì˜ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê°’ì„ ì‹¤ìˆ˜ë¡œ ê°€ì§€ëŠ” 2ì°¨ì› ë°°ì—´ 
```

```
# Out
array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.]])
```

#### ì‚°ìˆ  ì—°ì‚°

`ndarray`ëŠ” ê¸°ì´ˆì ì¸ ì‚°ìˆ  ì—°ì‚°ì„ ì§€ì›í•˜ëŠ”ë°,Â **`+`,`-`,Â `*`,Â `/`**ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ ì—°ì‚°ë“¤ì€ ê°™ì€ ìë¦¬ì˜ ì„±ë¶„ë¼ë¦¬ ì—°ì‚°ë˜ëŠ”Â *element-wise*Â ë°©ì‹ìœ¼ë¡œ, ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì›ì†Œì˜ ìˆ˜ê°€ ê°™ì„ ë•Œ ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. 

```python
x = np.array([1.0, 2.0, 3.0])
y = np.array([2.0, 4.0, 6.0])
print(x + y) # ì›ì†Œë³„ ë§ì…ˆ      [1.0 + 2.0, 2.0 + 4.0, 3.0 + 6.0]
print(x - y) # ì›ì†Œë³„ ëº„ì…ˆ      [1.0 - 2.0, 2.0 - 4.0, 3.0 - 6.0]
print(x * y) # ì›ì†Œë³„ ê³±ì…ˆ      [1.0 * 2.0, 2.0 * 4.0, 3.0 * 6.0]
print(x / y) # ì›ì†Œë³„ ë‚˜ëˆ—ì…ˆ    [1.0 / 2.0, 2.0 / 4.0, 3.0 / 6.0]
```

ë‹¤ë§Œ ë‹¤ìŒ *ê·¸ë¦¼1*ê³¼ ê°™ì´Â **boradcasting** ì´ë¼ëŠ” ë°©ë²•ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ `ndarray` ê°„ì—ë„ ì—°ì‚°ì´ ê°€ëŠ¥í•œ ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë³¸ì¸ì˜ ì˜ë„ì—ë§Œ ë§ê²Œ ì‚¬ìš©ëœë‹¤ë©´ ë°°ì—´ì˜ shapeë¥¼ ìˆ˜ì •í•  í•„ìš”ì—†ì´ ë°”ë¡œ ì‚°ìˆ  ì—°ì‚°ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

![Machine%20Learning%20from%20Scratch%20-%201%20Introduction%20153c58700b2f480491f782c37a9209e0/Untitled.png](Machine%20Learning%20from%20Scratch%20-%201%20Introduction%20153c58700b2f480491f782c37a9209e0/Untitled.png)

ê·¸ë¦¼1. broadcastingì˜ ì˜ˆì‹œ [ì¶œì²˜: "[Computation on Arrays: Broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html)"]

```python
x = np.arange(3)
x + 5 # [0 + 5, 1 + 5, 2 + 5]
```

#### N ì°¨ì› ë°°ì—´

*ê·¸ë¦¼2* ì—ì„œëŠ” ë¨¸ì‹ ëŸ¬ë‹ì„ ì ìš©í•˜ëŠ” ë°ì´í„°ê°€ ì´ë£¨ê³  ìˆëŠ” ëŒ€ë¶€ë¶„ì˜ í˜•íƒœë¥¼ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì†Œê°œí•˜ì§€ ì•Šê² ì§€ë§Œ, Pandasì˜ Series í˜•íƒœì˜ ë°ì´í„°ê°€ 1D arrayì™€ ê°™ì€ ëª¨ì–‘ì„ ë³´ì´ê²Œ ë  ê²ƒì´ê³ , ë°ì´í„° í”„ë ˆì„ì˜ ê²½ìš° 2D arrayì˜ í˜•íƒœë¥¼ ë³´ì—¬ì£¼ê²Œ ë  ê²ƒì…ë‹ˆë‹¤. 3D arrayì˜ ê²½ìš° ì£¼ë¡œ sequenceë¥¼ ê°€ì§„ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê²Œë  ë•Œ ìì£¼ ì ‘í•  ìˆ˜ ìˆëŠ” ì°¨ì›ì´ë©°, 4D arrayëŠ” ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê²Œëœë‹¤ë©´ ì ‘í•˜ê²Œë˜ì‹¤ ìˆ˜ë„ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ì—°ì¬í•˜ê²Œë  ë°ì´í„°ì˜ ìë£Œí˜•íƒœì— ë”°ë¥¸ ëª¨ë¸ë§ì—ì„œ í•´ë‹¹ ë¶€ë¶„ì„ ë”ìš± ìì„¸í•˜ê²Œ ë‹¤ë£° ìˆ˜ ìˆë„ë¡ í•˜ê² ê³ , ì—¬ê¸°ì„œëŠ” ê° ì°¨ì›ì„ ì–´ë–»ê²Œ ìƒì„±í•˜ëŠ”ì§€ë§Œ ë³´ì—¬ë“œë¦¬ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

![Machine%20Learning%20from%20Scratch%20-%201%20Introduction%20153c58700b2f480491f782c37a9209e0/Untitled%201.png](Machine%20Learning%20from%20Scratch%20-%201%20Introduction%20153c58700b2f480491f782c37a9209e0/Untitled%201.png)

ê·¸ë¦¼2. 1 ~ 3ì°¨ì›ì˜ ë°°ì—´ì— ëŒ€í•œ ì‹œê°ì  ìë£Œì™€ ì¶•(axis)ì˜ ìœ„ì¹˜ [ì¶œì²˜: "[íŒŒì´ì¬ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ Cheat Sheet: NumPy ê¸°ì´ˆ, ê¸°ë³¸](http://taewan.kim/post/numpy_cheat_sheet/)"]

```python
A = np.array([1, 2, 3])               # 1D array
B = np.array([[1, 2, 3], [4, 5, 6]])  # 2D array
C = np.array([[[1, 2, 3], [4, 5, 6]], # 3D array 
              [[7, 8, 9], [10, 11, 12]]])
print(A.shape, B.shape, C.shape) # (3,) (2, 3) (2, 2, 3)
```

### 2.2. ë²¡í„° ë° í–‰ë ¬ ì—°ì‚°

#### ë²¡í„° ì—°ì‚°

ë‹¤ìŒê³¼ ê°™ì€ ë²¡í„°ì˜ ì—°ì‚°ì„ ì–´ë–»ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆì„ì§€ì— ëŒ€í•´ì„œë„ í•œë²ˆ ê³ ë¯¼í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

$$\vec{a} = (1, 2, 3)$$

$$\vec{b} = (4, 5, 6)$$

$$\therefore \vec{a} + \vec{b} = (5, 7, 9)$$

`numpy`ë¥¼ í™œìš©í•˜ë©´ ìœ„ì—ì„œ ë´¤ë˜ ê²ƒì²˜ëŸ¼ ì•„ì£¼ ê°„ë‹¨í•˜ê²Œ ë²¡í„°ë¥¼ ìƒì„±í•˜ê³  ì—°ì‚°ì´ ê°€ëŠ¥í•œë°, ìš°ì„  ê¸°ë³¸ì ì¸ pythonë§Œì„ í†µí•´ì„œ êµ¬í˜„í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

- python `list`

```python
a = [1, 2, 3]
b = [4, 5, 6]

print(a + b)                                 # pythonì˜ ë¦¬ìŠ¤íŠ¸ì—ì„œëŠ” ì´ê²Œ ì•„ë‹ˆë‹¤.
print([a_i + b_i for a_i, b_i in zip(a, b)]) # zip í•¨ìˆ˜ë¥¼ ì¨ì„œ ë¬¶ê³ , list comprehension

def vector_sum(a, b):                        # í•¨ìˆ˜í™”
    return [a_i + b_i for a_i, b_i in zip(a, b)]
```

- `numpy` **ndarray**

```python
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

print(a + b)
```

#### í–‰ë ¬ ì—°ì‚°

ê¸°ì¡´ì˜ **ndarray** í˜•íƒœë¡œë„ ë¬¼ë¡  í–‰ë ¬ì— ê´€í•œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì§€ë§Œ, `numpy`ì—ì„œëŠ” ë³´ë‹¤ ê°„í¸í•˜ê²Œ í–‰ë ¬ì˜ ì—°ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” **matrix**ì˜ ë°ì´í„°í˜•ë„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. `np.array`ë¡œ ìƒì„±í•˜ë˜ ê²ƒì²˜ëŸ¼ `np.mat`ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ndarrayë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œë„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# numpy.ndarray
A = np.array([[1, 2], [2, 3]])
B = np.array([[1, 0], [2, 5]])

# numpy.matrix
C = np.mat(A) 
D = np.mat([[1, 0], [2, 5]])
```

ë‘ ë°ì´í„°í˜•ì˜ ì—°ì‚°ì„ ë¹„êµí•˜ë©´ ê½¤ë‚˜ ìœ ì˜ë¯¸í•œ ì°¨ì´ë¥¼ ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ ê¸°ë³¸ ê³±ì…ˆ ì—°ì‚°ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ì— ê´€í•œ ê²ƒì¸ë°, **ndarray**ì˜ ê²½ìš° ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ” ì›ì†Œê°„ì— ê³±ì„ í•˜ëŠ” element-wise ê³±ì…ˆì„ í•˜ì§€ë§Œ **matrix**ì˜ ê²½ìš° í–‰ë ¬ê³± ì—°ì‚°ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. **ndarray**ì—ì„œëŠ” í–‰ë ¬ê³±ì„ í•˜ê¸° ìœ„í•´ `np.dot(A, B)`ë¥¼ í•´ì•¼í•˜ë¯€ë¡œ ì¡°ê¸ˆ ë” ì§ê´€ì ìœ¼ë¡œ í–‰ë ¬ ì—°ì‚°ì„ í†µí•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶•ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” **matrix** ë°ì´í„°í˜•ì´ ì¡°ê¸ˆ ë” ì í•©í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

```python
print(A * B) # elemnet-wise mul
print(np.dot(A, B)) # or A.dot(B)
print(C * D) # í–‰ë ¬ì˜ ê³±
```

```
# Out 1
[[ 1  0]
 [ 4 15]]

# Out 2
[[ 5 10]
 [ 8 15]]

# Out 3
[[ 5 10]
 [ 8 15]]
```

í–‰ë ¬ê³±ì´ ì•„ë‹Œ í–‰ë ¬ ê°„ ë§ì…ˆ, ê·¸ë¦¬ê³  ìŠ¤ì¹¼ë¼ê³±ì€ ë‘ ë°ì´í„°í˜•ì—ì„œ ëª¨ë‘ ì˜ˆìƒí•˜ëŠ” ê²ƒì²˜ëŸ¼ ì‘ë™í•©ë‹ˆë‹¤.

```
print(A + B)
print(C + D)

print(3 * A)
print(3 * C)
```

```
# Out í–‰ë ¬ê°„ ë§ì…ˆ
[[2 2]
 [4 8]]

# Out ìŠ¤ì¹¼ë¼ê³±
[[3 6]
 [6 9]]

```

ë‹¤ìŒìœ¼ë¡œëŠ” íŠ¹ìˆ˜í•œ ì—°ì‚°ì¸ ì „ì¹˜(transpose) í–‰ë ¬, ì—­(inverse) í–‰ë ¬, ê·¸ë¦¬ê³  íŠ¹ì´ê°’ ë¶„í•´(singular value decomposition)ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. 

- ì „ì¹˜ í–‰ë ¬
    - ì „ì¹˜ í–‰ë ¬ì€ í–‰ë ¬ì˜ í–‰ê³¼ ì—´ì„ ë°”ê¾¸ëŠ” ê²ƒìœ¼ë¡œ `numpy`ì˜ ë°°ì—´ì˜ property ì¤‘ `T`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - í–¥í›„ ì˜ˆì¸¡ í˜¹ì€ ë¶„ë¥˜ ëª¨ë¸ì„ ë§Œë“¤ê²Œ ë  ë•Œ, **ì—¬ëŸ¬ feature ì—´ë“¤ì„ í¬í•¨í•œ í–‰ë ¬**ê³¼ **ê°€ì¤‘ì¹˜ í–‰ë ¬** ë“±ì„ ê³±í•˜ê¸° ìœ„í•´ ì¤‘ìš”í•˜ê²Œ í™œìš©ë  ê²ƒì…ë‹ˆë‹¤.

```python
 B.T
# Out
# array([[1, 4],
#        [2, 5],
#        [3, 6]])
```

- ì—­ í–‰ë ¬
    - **ndarray**ì˜ ê²½ìš° `numpy.linalg.inv()` ë¼ëŠ” í•¨ìˆ˜ë¥¼ í†µí•´ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - **matrix**ì˜ ê²½ìš° ê°„ë‹¨í•˜ê²Œ ì†ì„± ì¤‘ `I`ë¼ëŠ” ì†ì„±ì„ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.
    - âš ë‹¤ë§Œ ì£¼ì˜í•  ê²ƒì€ **ndarray**ì˜ ê²½ìš° shapeê°€ ì •ë°© í–‰ë ¬ì´ ì•„ë‹ˆë¼ë©´ pseudo inverseë¥¼ êµ¬í•˜ëŠ” `numpy.linalg.pinv()`ë¥¼ ì ìš©í•´ì•¼í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë¬¼ë¡  matrixì˜ ê²½ìš° ì •ë°© í–‰ë ¬ì´ ì•„ë‹ˆë”ë¼ë„ ë™ì¼í•˜ê²Œ `I` ì†ì„±ìœ¼ë¡œ ê°’ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜†

```python
np.linalg.inv(A)
# array([[-3.,  2.],
#        [ 2., -1.]])

C.I
# array([[-3.,  2.],
#        [ 2., -1.]])
```

- íŠ¹ì´ê°’ ë¶„í•´
    - `numpy.linalg.svd()`ë¼ëŠ” í•¨ìˆ˜ë¥¼ í†µí•´ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìœ„ì—ì„œ ì‚¬ìš©í–ˆë˜ `numpy`ì˜ ì„ í˜•ëŒ€ìˆ˜ ê´€ë ¨ ëª¨ë“ˆì€ í–¥í›„ì— ë” ìì„¸í•˜ê²Œ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

#### ì¸ë±ì‹±

- ì›ì†Œì— ëŒ€í•œ ì ‘ê·¼

```python
X = np.array([[51, 55], [14, 19], [0, 4]])
print(X)

print(f"0í–‰ {X[0]}")                  # 0í–‰ [51 55]
print(f"(0, 1) ìœ„ì¹˜ì˜ ì›ì†Œ {X[0][1]}")  # (0, 1) ìœ„ì¹˜ì˜ ì›ì†Œ 55
```

- for indexing

```python
# ëª¨ë“  rowë¥¼ ìˆœì„œëŒ€ë¡œ ì¶œë ¥
for row in X:
    print(row)
```

- bool indexing

```python
condition = X > 15   # 15 ì´ˆê³¼ì¸ ê°’ë§Œ subsettingí•˜ê³ ì í•  ê²½ìš°
print(condition)

X[condition]
```

## 3. Linear Algebra Application

### 3.1. ì„±ì  ì²˜ë¦¬

ìœ„ì™€ ê°™ì€ 1í•™ê¸°ì˜ ì„±ì í‘œê°€ ìˆê³ , (**ì¤‘ê°„**, **ê¸°ë§**, **ìˆ˜í–‰**)ì˜ ë°˜ì˜ ë¹„ìœ¨ì´ ( $35\%, 45\%, 20\%$) ì¼ë•Œì˜ ì´ì  ê³„ì‚°ì„ ì„ í˜•ëŒ€ìˆ˜ì ìœ¼ë¡œ ì²˜ë¦¬í•´ ë³´ê² ìŠµë‹ˆë‹¤. ìš°ì„  ìœ„ì˜ ì ìˆ˜ í…Œì´ë¸”ê³¼ ë°˜ì˜ ë¹„ìœ¨ì„ ê°ê°, í–‰ë ¬ê³¼  ë²¡í„°ë¡œ í‘œê¸°í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. (ê°„ë‹¨í•œ ì˜ˆì‹œì´ë¯€ë¡œ, ê° ì˜ì—­ë³„ ì´ì ì´ ëª¨ë‘ 100ì ì´ë¼ ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤) 

$$X =
\begin{pmatrix}
100 & 50 & 90 \\\\
70 & 85 & 80 \\\\
45 & 75 & 100
\end{pmatrix}
$$

$$\vec{p} = \big[0.35, 0.45,0.2 \big]$$

$$\vec{p}
\begin{bmatrix}
0.35 & 0.45 & 0.2 
\end{bmatrix} ^T$$

ë”°ë¼ì„œ í–‰ë ¬ê³¼ ë²¡í„°ì˜ ë‚´ì (inner product)ì„ í†µí•´ ë¹„ë¡œì†Œ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆê²Œë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

$$X \cdot \vec{p} =
\begin{pmatrix}
100 & 50 & 90 \\
70 & 85 & 80 \\
45 & 75 & 100
\end{pmatrix} \cdot
(0.35, 0.45, 0.2)
$$

```python
import numpy as np 

X = np.mat([[100, 50, 90],
            [70, 85, 80],
            [45, 75, 100]])

p = np.mat([0.35, 0.45, 0.2])

print(X * p.T)
```

### 3.2. ê¸°ìˆ  í†µê³„**(Descriptive statistics)**

> **ê¸°ìˆ  í†µê³„**ëŠ” ì •ë³´ ìˆ˜ì§‘ì˜ íŠ¹ì§•ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì„¤ëª…í•˜ê±°ë‚˜ ìš”ì•½í•˜ëŠ” ìš”ì•½ í†µê³„ì…ë‹ˆë‹¤[[1]](https://en.wikipedia.org/wiki/Descriptive_statistics#cite_note-1). ì¦‰, ë°ì´í„°ë¥¼ ìš”ì•½, ì„¤ëª…í•˜ëŠ”ë° ì´ˆì ì´ ë§ì¶”ì–´ì ¸ ìˆìœ¼ë©° ë‹¤ìŒê³¼ ê°™ì´ í¬ê²Œ 2ê°€ì§€ ê¸°ë²•ì´ ìˆìŠµë‹ˆë‹¤.

1. ì§‘ì¤‘í™” ê²½í–¥ (Central tendency): ë°ì´í„°ê°€ ì–´ë–¤ ê°’ì— ì§‘ì¤‘ë˜ì–´ ìˆëŠ”ê°€?
    - í‰ê· (Mean, Average)
2. ë¶„ì‚°ë„(Variation): ë°ì´í„°ê°€ ì–´ë–»ê²Œ í¼ì ¸ ìˆëŠ”ê°€?
    - ë¶„ì‚°(Variance), í‘œì¤€í¸ì°¨(Standard Deviation)

ë°ì´í„°ì˜ ê°¯ìˆ˜ê°€ $n$ê°œ ì´ê³ , ë°ì´í„°ì˜ ê° ì„±ë¶„ì„ $d_i$ë¡œ í‘œí˜„í•  ë•Œ,

$$\text{Mean} = \cfrac{d_1 + d_2 + \cdots + d_n}{n} = \cfrac{\sum d_i}{n} = \bar{d}$$

$$\begin{aligned} \text{Variance} &= \cfrac{(d_1 - \bar{d})^2 + (d_2 - \bar{d})^2 + \cdots + (d_n - \bar{d})^2}{n - 1} \\ &= \cfrac{\sum (d_i - \bar{d})^2}{n - 1} = \sigma^2\end{aligned}$$

$$\text{Standard Deviation} = \sqrt{\sigma^2} = \sigma$$

ê·¸ëŸ¬ë©´ ë¨¸ì‹ ëŸ¬ë‹ì˜ ëŒ€í‘œì ì¸ ì˜ˆì œ ë°ì´í„° ì¤‘ í•˜ë‚˜ì¸ ë¶“ê½ƒ(iris) ë°ì´í„°ì—ì„œ ê¸°ìˆ  í†µê³„ëŸ‰ì„ ì§ì ‘ êµ¬í•´ë³´ëŠ” ê³¼ì •ì„ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

```python
d = data.iloc[:, 1:5].values

n = d.shape[0]
data_mean = d.sum(axis=0)/n
print("ê° ì—´ì˜ í‰ê· : {}".format(data_mean))

data_var = ((d - data_mean)**2 ).sum(axis=0)/(n - 1)
print("ê° ì—´ì˜ ë¶„ì‚°: {}".format(data_var))

data_std = data_var**(1/2)
print("ê° ì—´ì˜ í‘œì¤€í¸ì°¨: {}".format(data_std))
```

### 3.3. ê¸°ìš¸ê¸° í•˜ê°•ë²•(Gradient Descent Method)

> ê¸°ìš¸ê¸° í•˜ê°•ë²•ì´ë€ ì–´ë–¤ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ ì°¾ê¸° ìœ„í•´ ê·¸ í•¨ìˆ˜ë¥¼ 1ì°¨ ë¯¸ë¶„í•´ì„œ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œ í›„ ê·¸ ê¸°ìš¸ê¸°ë¥¼ ë”°ë¼ ë‚´ë ¤ê°€ë©´ì„œ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ë°©ë²•ì´ë‹¤. -ë°ì´í„° ì• ë„ë¦¬í‹±ìŠ¤(ì´ì¬ì‹, 2020)-

![Machine%20Learning%20from%20Scratch%20-%201%20Introduction%20153c58700b2f480491f782c37a9209e0/Untitled%202.png](Machine%20Learning%20from%20Scratch%20-%201%20Introduction%20153c58700b2f480491f782c37a9209e0/Untitled%202.png)

ê·¸ë¦¼3. ë¹„ìš©(Cost) í•¨ìˆ˜ ë˜ëŠ” ì†ì‹¤(Loss) í•¨ìˆ˜ì—ì„œì˜ ê¸°ìš¸ê¸° í•˜ê°•ë²• ì ìš© 

ë§ˆì§€ë§‰ìœ¼ë¡œ ì†Œê°œí•´ë“œë¦´ ì‘ìš© ë°©ì•ˆ ì¤‘ í•˜ë‚˜ëŠ” ì•ìœ¼ë¡œ ìƒë‹¹íˆ ì¤‘ìš”í•˜ê²Œ ì“°ì´ëŠ” ê¸°ìš¸ê¸° í•˜ê°•ë²•(or ê²½ì‚¬ í•˜ê°•ë²•)ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” ëª©ì ìœ¼ë¡œ í•˜ëŠ” Task(e.g., ì˜ˆì¸¡, ë¶„ë¥˜ ë“±)ë¥¼ ì˜ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ ì™„ë²½í•œ ë‹µì„ ì°¾ê¸° ë³´ë‹¤ëŠ” ìµœì ì˜, ì¶©ë¶„í•œ ë‹µì„ ì°¾ê¸° ìœ„í•´ ë…¸ë ¥í•©ë‹ˆë‹¤. ê¸°ìš¸ê¸° í•˜ê°•ë²•ì€ ì´ëŸ¬í•œ ëª©ì ì— ë¶€í•©í•œ ì •ë‹µì˜ ê·¼ì‚¬ì¹˜(Approximation)ë¥¼ ì°¾ê¸° ìœ„í•´ ë‹µì•ˆì„ ìˆ˜ì •í•´ë‚˜ê°€ëŠ” ë°©ë²•ë¡  ì¤‘ í•˜ë‚˜ë¡œ ì´í•´í•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì‹œë¦¬ì¦ˆì˜ ë°”ë¡œ ë‹¤ìŒ ê¸€ ë¶€í„°ëŠ” *ê·¸ë¦¼3*ê³¼ ê°™ì´ ê¸°ìš¸ê¸° í•˜ê°•ë²•ì„ í†µí•´ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •í•´ë‚˜ê°€ëŠ” ê³¼ì •ì„ ë³´ì—¬ë“œë¦´ ê²ƒì´ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ì¸ ì´í•´ë¥¼ ìœ„í•´ ê°„ë‹¨í•œ ì˜ˆì‹œë§Œ ë‹¤ë£¨ì–´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ìš°ì„  ì´ì°¨ í•¨ìˆ˜ $f(x) = x^2 -2x + 3$ì˜ ìµœì†Ÿê°’ì„ êµ¬í•˜ëŠ” ìƒí™©ì„ ê°€ì •í•´ë³´ê² ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì—¬ê¸°ì„œ ì‹ì„ $f(x) = (x - 1)^2 + 2$ ë¡œ ë³€í™˜í•  ìˆ˜ ìˆê³ , ë”°ë¼ì„œ ìµœì†Ÿê°’ì´ $2$ë¼ëŠ” ê²ƒì„ ê¸ˆë°© êµ¬ì‚´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì»´í“¨í„°ì˜ ìˆ˜ì¹˜ê³„ì‚°ìœ¼ë¡œëŠ” ì´ê²ƒì„ ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ì§€ê°€ ì§ë©´í•œ ë¬¸ì œìƒí™©ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ìš¸ê¸° í•˜ê°•ë²•ì˜ ì•„ì´ë””ì–´ë¡œ ë¬¸ì œë¥¼ í‘¼ë‹¤ë©´ ë¬´ì‘ìœ„ì˜ ìœ„ì¹˜ì—ì„œ ì‹œì‘í•˜ê³ , í•´ë‹¹ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ê¸°ê°€ ê°€íŒŒë¥´ë‹¤ë©´ ê°’ì„ í¬ê²Œ ìˆ˜ì •í•˜ê³  ê¸°ìš¸ê¸°ê°€ ì™„ë§Œí•˜ë‹¤ë©´ ê°’ì„ ì‘ê²Œ ìˆ˜ì •í•˜ëŠ” ì „ëµìœ¼ë¡œ ì„¤ëª…ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

$$x_{t+1} = x_t - \alpha \cdot \cfrac{df(x_t)}{dx_t} $$

ìœ„ ì‹ì„ í†µí•´ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì€ ìƒˆë¡œìš´ ê°’ì˜ ìˆ˜ì •ì„ ìœ„í•´ì„œ ê¸°ìš¸ê¸°ë¥¼ ì–¼ë§ˆë§Œí¼ ë°˜ì˜í•˜ì—¬ ìˆ˜ì •í•  ê²ƒì´ê³ , ìµœì†Ÿê°’ì„ êµ¬í•˜ê³ ì í•˜ëŠ” í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ë¥¼ ì•Œì•„ì•¼í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ë„í•¨ìˆ˜ì˜ ê²½ìš° $f'(x) = g(x) = 2x - 2$ë¡œ ë¹„êµì  ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆê³ , $\alpha = 0.3$ìœ¼ë¡œ ì„¤ì •í•´ë³´ê² ìŠµë‹ˆë‹¤. 

```python
def f(x):
    return x**2 - 2*x + 3

def g(x):
    return 2*x -2

x = np.array([10]) # ì‹œì‘ ìœ„ì¹˜ x=10
alpha = 0.3

for i in range(10):
    x = x - alpha * g(x)
    print(f"ìˆ˜ì •ëœ x: {x} \t í•´ë‹¹ í•¨ìˆ˜ê°’: {f(x)}") 

# ìˆ˜ì •ëœ x: [4.6] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [14.96]
# ìˆ˜ì •ëœ x: [2.44] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [4.0736]
# ìˆ˜ì •ëœ x: [1.576] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [2.331776]
# ìˆ˜ì •ëœ x: [1.2304] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [2.05308416]
# ìˆ˜ì •ëœ x: [1.09216] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [2.00849347]
# ìˆ˜ì •ëœ x: [1.036864] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [2.00135895]
# ìˆ˜ì •ëœ x: [1.0147456] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [2.00021743]
# ìˆ˜ì •ëœ x: [1.00589824] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [2.00003479]
# ìˆ˜ì •ëœ x: [1.0023593] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [2.00000557]
# ìˆ˜ì •ëœ x: [1.00094372] 	 í•´ë‹¹ í•¨ìˆ˜ê°’: [2.00000089]
```

ì´ë¡œì¨ ìµœì†Ÿê°’ì„ ë§Œì¡±í•˜ëŠ” ì¡°ê±´ì´ $x=1$ ì¼ ê²½ìš°ì´ë©°, ê°’ì´ $2$ë¡œ ê·¼ì‚¬í•´ê°„ë‹¤ëŠ” ì‚¬ì‹¤ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. x ì˜ ì‹œì‘ê°’ì´ë‚˜ $\alpha$ë¥¼ ë‹¤ì–‘í•œ ê°’ìœ¼ë¡œ ì¡°ì ˆí•˜ë©° ì‹¤í—˜ì„ í•´ë³´ë”ë¼ë„, ì ë‹¹í•œ ë°˜ë³µë§Œ ì£¼ì–´ì§€ê²Œ ë˜ë©´ ì•ì„  ìµœì†Ÿê°’ì„ ë§Œì¡±í•˜ëŠ” ì¡°ê±´ìœ¼ë¡œ ìˆ˜ë ´í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

# ë§ˆì¹˜ë©°

í•­ìƒ ì •ë¦¬í•´ì•¼ì§€í•˜ê³  ë§ˆìŒë§Œ ë¨¹ì€ ë’¤ì— ì½”ë“œë§Œ ëŒ€ì¶© ì‘ì„±í–ˆë˜ ë¶€ë¶„ì„ ë“œë””ì–´ í•œê³³ì— ëª¨ì•„ ì •ë¦¬í•˜ê¸° ì‹œì‘í–ˆë‹¤ëŠ” ì ì—ì„œ ê°íšŒê°€ ìƒˆë¡œìš´ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¼ë‹¨ ì‹œì‘ì„ í–ˆë‹¤ëŠ” ì ì—ì„œ ì ˆë°˜(?)ì€ í–ˆë‹¤ëŠ” ì•ˆë„ê°ë„ë“¤ì§€ë§Œ, ì‹œë¦¬ì¦ˆì˜ ì²«ê¸€ì´ì ì˜¤ëœë§Œì— ì‘ì„±í•˜ëŠ” ê¸€ì´ë‹¤ë³´ë‹ˆ ë‘ì„œ ì—†ê³  ë¶€ì¡±í•œ ë¶€ë¶„ì´ ë§ê¸° ë•Œë¬¸ì—, ê¸€ì—ì„œ ì œê°€ ì˜¤ë¥˜ë¥¼ ë²”í•œ ë¶€ë¶„ì´ë‚˜ ì¶”ê°€ì ìœ¼ë¡œ í•„ìš”í•œ ë¶€ë¶„ì— ëŒ€í•œ ì¶©ê³ ë‚˜ ì¡°ì–¸ì€ í•­ìƒ ê°ì‚¬íˆ ë°›ê³  ë°˜ì˜í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ê¸´ ê¸€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ˜‚

# Reference

- Coursera Machine Learning
    - Week1: Linear Regression with One Variable
    - Week1: Linear Algebra Review
- Numpy
    - ë°‘ë°”ë‹¥ ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ Ch01 [[github]](https://github.com/WegraLee/deep-learning-from-scratch/tree/master/ch01)
    - numpy ê³µì‹ ë¬¸ì„œ quickstart [[doc]](https://numpy.org/doc/stable/user/quickstart.html)
- ì‘ìš© íŒŒíŠ¸
    - ê¸°ìˆ í†µê³„ì™€ ì¶”ë¦¬ í†µê³„ë€ ë¬´ì—‡ì¸ê°€? [[tistory]](https://drhongdatanote.tistory.com/25)

# Machine-Learning

## 2018ë…„ 1ì›” ì•ˆìƒí˜¸

---

ì´ ìë£ŒëŠ” ì „ í™ì½© ê³¼ê¸°ëŒ€ êµìˆ˜ë‹˜ì´ì‹  ê¹€ì„±í›ˆ êµìˆ˜ë‹˜ì˜ ***[ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹](https://www.youtube.com/playlist?list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)*** ê°•ì˜ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ì—¬ ***[ë°‘ë°”ë‹¥ ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹](http://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198)***ê³¼ ì•¤ë“œë¥˜ ì‘ êµìˆ˜ë‹˜ì˜ ***[Machine Learning](https://www.coursera.org/learn/machine-learning)***ìœ¼ë¡œ ë¶€ì¡±í•œ ë¶€ë¶„ì€ ë³´ì™„í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤

êµ¬í˜„ëœ ì–¸ì–´ëŠ” **Python**ì´ê³ , ê¸°ì´ˆ ê°œë…ì€ `numpy` ìœ„ì£¼ì˜ ì„ í˜• ëŒ€ìˆ˜ ê°œë…ì„ í¬í•¨í•˜ì˜€ìœ¼ë©° ì´í•´ëœ ê°œë…ì„ ë°”íƒ•ìœ¼ë¡œ `TensorFlow`ì—ì„œ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

`.ipynb`ì—ì„œ ì‘ì„±í•œ **Markdown**ì—ì„œ html ì½”ë“œë¥¼ ì‚½ì…í•˜ëŠ” ê³¼ì •ì—ì„œ github Viewerê°€ ì˜¤ë¥˜ë¡œ ì œëŒ€ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ìˆì–´ì„œ ì´ [ì‚¬ì´íŠ¸](http://nbviewer.jupyter.org/github/sanghoho/Machine-Learning/tree/master/)ë¥¼ í†µí•´ íŒŒì¼ì„ ë³´ì‹œë©´ ì˜¨ì „í•˜ê²Œ íŒŒì¼ì„ ì—´ëŒí•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!

# Machine Learning and Cyber Security

## 2018ë…„ 11ì›” 02ì¼

> ë¨¸ì‹ ëŸ¬ë‹(Machine Learning; ê¸°ê³„í•™ìŠµ)ê³¼ ë³´ì•ˆ(Security)

*ë§ˆìŠ¤í„° ì•Œê³ ë¦¬ì¦˜(í˜ë“œë¡œ ë„ë°ê³ ìŠ¤, 2015)* ì— ë”°ë¥´ë©´ ë¨¸ì‹ ëŸ¬ë‹ì˜ ë°©ë²•ë¡ ì„ ê¸°ì¤€ìœ¼ë¡œ

**ê¸°í˜¸ì£¼ì˜**, **ì—°ê²°ì£¼ì˜**, **ì§„í™”ì£¼ì˜**, **ë² ì´ì§€ì•ˆ**, **ìœ ì¶”ì£¼ì˜**, ì´ë ‡ê²Œ 5ê°€ì§€ ì—°êµ¬ ë¶„ì•¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

ìµœê·¼ ë„ë¦¬ ì•Œë ¤ì§„ **ë”¥ëŸ¬ë‹(Deep Learning)** ì€ **ì—°ê²°ì£¼ì˜**ì˜ í•œ ë°©ë²•ë¡ ìœ¼ë¡œ ì´ ë°©ë²•ë¡ ì— ì´ˆì ì„ ë§ì¶”ì–´ ë¬¸ì„œë¥¼ ì •ë¦¬í•˜ë„ë¡ í•œë‹¤.

---

- êµ¬ì„±
    - `/ml`: The Basic Understanding of **Machine Learning**
    - `/cs`: The Basic Understanding of **Cyber Security**
    - `/mlforcs`: The Application of **Machine Learning** on **Security**

---

## Reference

- [Awesome Machine Learning for Cyber Security](https://github.com/jivoi/awesome-ml-for-cybersecurity)
